{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing lol\n"
     ]
    }
   ],
   "source": [
    "print(\"testing lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/CompVis/taming-transformers\n",
    "%cd taming-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-13 15:15:21--  https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1\n",
      "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
      "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://heibox.uni-heidelberg.de/seafhttp/files/653696ed-7556-4d77-a3dd-4bbc6f828adc/last.ckpt [following]\n",
      "--2024-11-13 15:15:22--  https://heibox.uni-heidelberg.de/seafhttp/files/653696ed-7556-4d77-a3dd-4bbc6f828adc/last.ckpt\n",
      "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 957954257 (914M) [application/octet-stream]\n",
      "Saving to: ‘logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt’\n",
      "\n",
      "logs/vqgan_imagenet 100%[===================>] 913.58M  6.70MB/s    in 2m 11s  \n",
      "\n",
      "2024-11-13 15:17:33 (6.99 MB/s) - ‘logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt’ saved [957954257/957954257]\n",
      "\n",
      "--2024-11-13 15:17:33--  https://heibox.uni-heidelberg.de/f/6ecf2af6c658432c8298/?dl=1\n",
      "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
      "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://heibox.uni-heidelberg.de/seafhttp/files/623710d5-9555-4ca2-8bfc-7697acaf713b/model.yaml [following]\n",
      "--2024-11-13 15:17:35--  https://heibox.uni-heidelberg.de/seafhttp/files/623710d5-9555-4ca2-8bfc-7697acaf713b/model.yaml\n",
      "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 645 [application/octet-stream]\n",
      "Saving to: ‘logs/vqgan_imagenet_f16_1024/configs/model.yaml’\n",
      "\n",
      "logs/vqgan_imagenet 100%[===================>]     645  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-13 15:17:36 (271 MB/s) - ‘logs/vqgan_imagenet_f16_1024/configs/model.yaml’ saved [645/645]\n",
      "\n",
      "--2024-11-13 15:17:37--  https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1\n",
      "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
      "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://heibox.uni-heidelberg.de/seafhttp/files/3a935b70-f279-4ca5-914d-26b1d3d257ad/last.ckpt [following]\n",
      "--2024-11-13 15:17:38--  https://heibox.uni-heidelberg.de/seafhttp/files/3a935b70-f279-4ca5-914d-26b1d3d257ad/last.ckpt\n",
      "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 980092370 (935M) [application/octet-stream]\n",
      "Saving to: ‘logs/vqgan_imagenet_f16_16384/checkpoints/last.ckpt’\n",
      "\n",
      "logs/vqgan_imagenet 100%[===================>] 934.69M  8.39MB/s    in 2m 7s   \n",
      "\n",
      "2024-11-13 15:19:45 (7.39 MB/s) - ‘logs/vqgan_imagenet_f16_16384/checkpoints/last.ckpt’ saved [980092370/980092370]\n",
      "\n",
      "--2024-11-13 15:19:45--  https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1\n",
      "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
      "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://heibox.uni-heidelberg.de/seafhttp/files/0720808b-515a-4228-9b2a-9a7697f57195/model.yaml [following]\n",
      "--2024-11-13 15:19:46--  https://heibox.uni-heidelberg.de/seafhttp/files/0720808b-515a-4228-9b2a-9a7697f57195/model.yaml\n",
      "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 692 [application/octet-stream]\n",
      "Saving to: ‘logs/vqgan_imagenet_f16_16384/configs/model.yaml’\n",
      "\n",
      "logs/vqgan_imagenet 100%[===================>]     692  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-13 15:19:47 (245 MB/s) - ‘logs/vqgan_imagenet_f16_16384/configs/model.yaml’ saved [692/692]\n",
      "\n",
      "--2024-11-13 15:19:48--  https://heibox.uni-heidelberg.de/f/34a747d5765840b5a99d/?dl=1\n",
      "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
      "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://heibox.uni-heidelberg.de/seafhttp/files/5d692718-7bbd-4603-ad3c-c5ea91d8cac9/last.ckpt [following]\n",
      "--2024-11-13 15:19:49--  https://heibox.uni-heidelberg.de/seafhttp/files/5d692718-7bbd-4603-ad3c-c5ea91d8cac9/last.ckpt\n",
      "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 376581823 (359M) [application/octet-stream]\n",
      "Saving to: ‘logs/vqgan_gumbel_f8/checkpoints/last.ckpt’\n",
      "\n",
      "logs/vqgan_gumbel_f 100%[===================>] 359.14M  7.89MB/s    in 51s     \n",
      "\n",
      "2024-11-13 15:20:40 (7.07 MB/s) - ‘logs/vqgan_gumbel_f8/checkpoints/last.ckpt’ saved [376581823/376581823]\n",
      "\n",
      "--2024-11-13 15:20:41--  https://heibox.uni-heidelberg.de/f/b24d14998a8d4f19a34f/?dl=1\n",
      "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
      "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://heibox.uni-heidelberg.de/seafhttp/files/5aca3f6b-15f1-45ba-a0fa-8e4899fa78a2/model.yaml [following]\n",
      "--2024-11-13 15:20:42--  https://heibox.uni-heidelberg.de/seafhttp/files/5aca3f6b-15f1-45ba-a0fa-8e4899fa78a2/model.yaml\n",
      "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 745 [application/octet-stream]\n",
      "Saving to: ‘logs/vqgan_gumbel_f8/configs/model.yaml’\n",
      "\n",
      "logs/vqgan_gumbel_f 100%[===================>]     745  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-13 15:20:42 (263 MB/s) - ‘logs/vqgan_gumbel_f8/configs/model.yaml’ saved [745/745]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download a VQGAN with f=16 (16x compression per spatial dimension) and with a codebook with 1024 entries\n",
    "!mkdir -p logs/vqgan_imagenet_f16_1024/checkpoints\n",
    "!mkdir -p logs/vqgan_imagenet_f16_1024/configs\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1' -O 'logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt'\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/6ecf2af6c658432c8298/?dl=1' -O 'logs/vqgan_imagenet_f16_1024/configs/model.yaml'\n",
    "\n",
    "# download a VQGAN with f=16 (16x compression per spatial dimension) and with a larger codebook (16384 entries)\n",
    "!mkdir -p logs/vqgan_imagenet_f16_16384/checkpoints\n",
    "!mkdir -p logs/vqgan_imagenet_f16_16384/configs\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1' -O 'logs/vqgan_imagenet_f16_16384/checkpoints/last.ckpt'\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1' -O 'logs/vqgan_imagenet_f16_16384/configs/model.yaml'\n",
    "\n",
    "# download a VQGAN with f=8 (8x compression per spatial dimension) and a larger codebook-size with 8192 entries\n",
    "!mkdir -p logs/vqgan_gumbel_f8/checkpoints\n",
    "!mkdir -p logs/vqgan_gumbel_f8/configs\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/34a747d5765840b5a99d/?dl=1' -O 'logs/vqgan_gumbel_f8/checkpoints/last.ckpt'\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/b24d14998a8d4f19a34f/?dl=1' -O 'logs/vqgan_gumbel_f8/configs/model.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch==1.7.0\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pytorch==1.7.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision==0.8.1 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (0.8.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from torchvision==0.8.1) (10.4.0)\n",
      "Requirement already satisfied: torch==1.7.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from torchvision==0.8.1) (1.7.0)\n",
      "Requirement already satisfied: numpy in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from torchvision==0.8.1) (1.24.4)\n",
      "Requirement already satisfied: dataclasses in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from torch==1.7.0->torchvision==0.8.1) (0.6)\n",
      "Requirement already satisfied: typing-extensions in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from torch==1.7.0->torchvision==0.8.1) (4.12.2)\n",
      "Requirement already satisfied: future in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from torch==1.7.0->torchvision==0.8.1) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pytorch-lightning==1.0.8 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (1.0.8)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from pytorch-lightning==1.0.8) (2024.10.0)\n",
      "Requirement already satisfied: torch>=1.3 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from pytorch-lightning==1.0.8) (1.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from pytorch-lightning==1.0.8) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from pytorch-lightning==1.0.8) (4.67.0)\n",
      "Requirement already satisfied: numpy>=1.16.4 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from pytorch-lightning==1.0.8) (1.24.4)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from pytorch-lightning==1.0.8) (1.0.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from pytorch-lightning==1.0.8) (2.14.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.7)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (5.28.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.0.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (2.36.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (71.0.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (0.43.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (2.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.0.8) (0.7.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (0.4.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (8.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.19.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (0.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (3.2.2)\n",
      "Requirement already satisfied: dataclasses in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from torch>=1.3->pytorch-lightning==1.0.8) (0.6)\n",
      "Requirement already satisfied: typing-extensions in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from torch>=1.3->pytorch-lightning==1.0.8) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.0.8) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# SKIP? Minimum requirements (later version)\n",
    "# %pip install torch==1.13.1\n",
    "# %pip install torchvision==0.14.1\n",
    "\n",
    "# %pip install torchvision==0.10.1\n",
    "# %pip install torch==1.9.1\n",
    "\n",
    "%pip install pytorch==1.7.0\n",
    "%pip install torchvision==0.8.1\n",
    "%pip install pytorch-lightning==1.0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 einops>=0.3.0\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# also disable grad to save memory\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some loading utilities\n",
    "import yaml\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "\n",
    "def load_config(config_path, display=False):\n",
    "  config = OmegaConf.load(config_path)\n",
    "  if display:\n",
    "    print(yaml.dump(OmegaConf.to_container(config)))\n",
    "  return config\n",
    "\n",
    "def load_vqgan(config, ckpt_path=None, is_gumbel=False):\n",
    "  if is_gumbel:\n",
    "    model = GumbelVQ(**config.model.params)\n",
    "  else:\n",
    "    model = VQModel(**config.model.params)\n",
    "  if ckpt_path is not None:\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "  return model.eval()\n",
    "\n",
    "def preprocess_vqgan(x):\n",
    "  x = 2.*x - 1.\n",
    "  return x\n",
    "\n",
    "def custom_to_pil(x):\n",
    "  x = x.detach().cpu()\n",
    "  x = torch.clamp(x, -1., 1.)\n",
    "  x = (x + 1.)/2.\n",
    "  x = x.permute(1,2,0).numpy()\n",
    "  x = (255*x).astype(np.uint8)\n",
    "  x = Image.fromarray(x)\n",
    "  if not x.mode == \"RGB\":\n",
    "    x = x.convert(\"RGB\")\n",
    "  return x\n",
    "\n",
    "def reconstruct_with_vqgan(x, model):\n",
    "  # could also use model(x) for reconstruction but use explicit encoding and decoding here\n",
    "  z, _, [_, _, indices] = model.encode(x)\n",
    "  print(f\"VQGAN --- {model.__class__.__name__}: latent shape: {z.shape[2:]}\")\n",
    "  xrec = model.decode(z)\n",
    "  return xrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nora/miniconda3/envs/llama/lib/python3.8/site-packages/torch/cuda/__init__.py:104: UserWarning: \n",
      "NVIDIA GeForce RTX 4090 with CUDA capability sm_89 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 4090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "# LOAD VQGAN MODELS HERE\n",
    "config1024 = load_config(\"logs/vqgan_imagenet_f16_1024/configs/model.yaml\", display=False)\n",
    "config16384 = load_config(\"logs/vqgan_imagenet_f16_16384/configs/model.yaml\", display=False)\n",
    "\n",
    "model1024 = load_vqgan(config1024, ckpt_path=\"logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt\").to(DEVICE)\n",
    "model16384 = load_vqgan(config16384, ckpt_path=\"logs/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n"
     ]
    }
   ],
   "source": [
    "# load f8 models\n",
    "config32x32 = load_config(\"logs/vqgan_gumbel_f8/configs/model.yaml\", display=False)\n",
    "model32x32 = load_vqgan(config32x32, ckpt_path=\"logs/vqgan_gumbel_f8/checkpoints/last.ckpt\", is_gumbel=True).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.8/site-packages/IPython/utils/_process_posix.py:148\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    149\u001b[0m flush \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.8/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_poll \u001b[38;5;241m=\u001b[39m use_poll\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.8/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc\u001b[38;5;241m.\u001b[39mpid\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.8/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPtyProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.8/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_read)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstall git+https://github.com/openai/DALL-E.git &> /dev/null\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2415\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2416\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2417\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2419\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2420\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2421\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.8/site-packages/IPython/core/magics/packaging.py:75\u001b[0m, in \u001b[0;36mPackagingMagics.pip\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     python \u001b[38;5;241m=\u001b[39m shlex\u001b[38;5;241m.\u001b[39mquote(python)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpython\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote: you may need to restart the kernel to use updated packages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.8/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.8/site-packages/IPython/utils/_process_posix.py:164\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    159\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241m.\u001b[39msendline(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/openai/DALL-E.git &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DALL-E\n",
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from dall_e          import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display, display_markdown\n",
    "\n",
    "font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf\", 22)\n",
    "\n",
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return PIL.Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "\n",
    "def preprocess(img, target_image_size=256, map_dalle=True):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    if map_dalle: \n",
    "      img = map_pixels(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def reconstruct_with_dalle(x, encoder, decoder, do_preprocess=False):\n",
    "  # takes in tensor (or optionally, a PIL image) and returns a PIL image\n",
    "  if do_preprocess:\n",
    "    x = preprocess(x)\n",
    "  z_logits = encoder(x)\n",
    "  z = torch.argmax(z_logits, axis=1)\n",
    "  \n",
    "  print(f\"DALL-E: latent shape: {z.shape}\")\n",
    "  z = F.one_hot(z, num_classes=encoder.vocab_size).permute(0, 3, 1, 2).float()\n",
    "\n",
    "  x_stats = decoder(z).float()\n",
    "  x_rec = unmap_pixels(torch.sigmoid(x_stats[:, :3]))\n",
    "  x_rec = T.ToPILImage(mode='RGB')(x_rec[0])\n",
    "\n",
    "  return x_rec\n",
    "\n",
    "\n",
    "def stack_reconstructions(input, x0, x1, x2, x3, titles=[]):\n",
    "  assert input.size == x1.size == x2.size == x3.size\n",
    "  w, h = input.size[0], input.size[1]\n",
    "  img = Image.new(\"RGB\", (5*w, h))\n",
    "  img.paste(input, (0,0))\n",
    "  img.paste(x0, (1*w,0))\n",
    "  img.paste(x1, (2*w,0))\n",
    "  img.paste(x2, (3*w,0))\n",
    "  img.paste(x3, (4*w,0))\n",
    "  for i, title in enumerate(titles):\n",
    "    ImageDraw.Draw(img).text((i*w, 0), f'{title}', (255, 255, 255), font=font) # coordinates, text, color, font\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster load times, download these files locally and use the local paths instead.\n",
    "encoder_dalle = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", DEVICE)\n",
    "decoder_dalle = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECONSTRUCT IMAGES \n",
    "titles=[\"Input\", \"DALL-E dVAE (f8, 8192)\", \"VQGAN (f8, 8192)\", \n",
    "        \"VQGAN (f16, 16384)\", \"VQGAN (f16, 1024)\"]\n",
    "\n",
    "def reconstruction_pipeline(url, size=320):\n",
    "  x_dalle = preprocess(download_image(url), target_image_size=size, map_dalle=True)\n",
    "  x_vqgan = preprocess(download_image(url), target_image_size=size, map_dalle=False)\n",
    "  x_dalle = x_dalle.to(DEVICE)\n",
    "  x_vqgan = x_vqgan.to(DEVICE)\n",
    "  \n",
    "  print(f\"input is of size: {x_vqgan.shape}\")\n",
    "  x0 = reconstruct_with_vqgan(preprocess_vqgan(x_vqgan), model32x32)\n",
    "  x1 = reconstruct_with_vqgan(preprocess_vqgan(x_vqgan), model16384)\n",
    "  x2 = reconstruct_with_vqgan(preprocess_vqgan(x_vqgan), model1024)\n",
    "  x3 = reconstruct_with_dalle(x_dalle, encoder_dalle, decoder_dalle)\n",
    "  img = stack_reconstructions(custom_to_pil(preprocess_vqgan(x_vqgan[0])), x3, \n",
    "                              custom_to_pil(x0[0]), custom_to_pil(x1[0]), \n",
    "                              custom_to_pil(x2[0]), titles=titles)\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cu102\n",
      "Collecting torch==1.10.0\n",
      "  Using cached torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
      "Requirement already satisfied: typing-extensions in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from torch==1.10.0) (4.12.2)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.1\n",
      "    Uninstalling torch-1.9.1:\n",
      "      Successfully uninstalled torch-1.9.1\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torchvision==0.11.0\n",
      "  Downloading torchvision-0.11.0-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.3 MB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/nora/miniconda3/envs/llama/lib/python3.8/site-packages (from torchvision==0.11.0) (10.4.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.0+cu102 (from torchvision)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch==1.10.0+cu102\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "# %pip install torchvision==0.10.1\n",
    "# %pip install torch==1.9.1\n",
    "\n",
    "%pip install pytorch=1.7.0\n",
    "%pip install torchvision=0.8.1\n",
    "%pytorch-lightning==1.0.8\n",
    "# %pip install torchvision==0.11.0 ##### WORKING ON THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n",
      "input is of size: torch.Size([1, 3, 384, 384])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Notebook Example\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mreconstruction_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://heibox.uni-heidelberg.de/f/7bb608381aae4539ba7a/?dl=1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mreconstruction_pipeline\u001b[0;34m(url, size)\u001b[0m\n\u001b[1;32m      9\u001b[0m x_vqgan \u001b[38;5;241m=\u001b[39m x_vqgan\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput is of size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_vqgan\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m x0 \u001b[38;5;241m=\u001b[39m reconstruct_with_vqgan(\u001b[43mpreprocess_vqgan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_vqgan\u001b[49m\u001b[43m)\u001b[49m, model32x32)\n\u001b[1;32m     13\u001b[0m x1 \u001b[38;5;241m=\u001b[39m reconstruct_with_vqgan(preprocess_vqgan(x_vqgan), model16384)\n\u001b[1;32m     14\u001b[0m x2 \u001b[38;5;241m=\u001b[39m reconstruct_with_vqgan(preprocess_vqgan(x_vqgan), model1024)\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36mpreprocess_vqgan\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_vqgan\u001b[39m(x):\n\u001b[0;32m---> 24\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m2.\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m     25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "# Notebook Example\n",
    "reconstruction_pipeline(url='https://heibox.uni-heidelberg.de/f/7bb608381aae4539ba7a/?dl=1', size=384)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
